# -*- coding: utf-8 -*-
"""Mini-Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1of2DomU8E41SKNY0Jt2c-vjkTtww9Fmr

# **Initial Setup**
"""

!gdown --id 1-N8aXiUxnB2dOpeDIU9y0wJP2ioLjKtl --output cv2.cpython-36m-x86_64-linux-gnu.so #enable GPU support

!gdown --id 19yjsdPXQ2DQURD1xqzYHrGTdhMbgGywL --output pedestrians.mp4
!gdown --id 1jgMqCnnZ5PYQxFHvJRp8W8URcQhohbJZ --output coco.names
!gdown --id 1UuzgjSOwLJHE6FrGjluwh9ORyknlJfBw --output yolov3.cfg
!gdown --id 1qlBKyUChyvkMc3YcSnc_4JpcxeS-93lY --output yolov3.weights

"""# **Importing the libraries**"""

import cv2 as cv #OpenCV Library
import numpy as np  #for handling arrays
from scipy.spatial import distance  #for cdist 
#from tqdm.std import tqdm  #for system progressbar
from tqdm.notebook import tqdm #for googl colab progressbar

"""# **Total frames counter**"""

def total_frames(file_name):
  cap = cv.VideoCapture(file_name)
  res = 0

  while True:
    ret,img = cap.read()

    if not ret: break
    res = res+1
                                          
  return res

"""# **YOLO object detection function**"""

def object_detection_YOLO(img,threshold,nms_threshold):
  # determine the output layers
  ln = net.getLayerNames()
  ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]
  # construct a blob from the image
  # blob is just a preprocessed image
  blob = cv.dnn.blobFromImage(img, 1/255.0, (416, 416), swapRB=True, crop=False)      #blob = boxes
  # blobs goes as the input to YOLO
  # inputting blob to the Neural Network
  net.setInput(blob)
  # t0 = time.time()
  outputs = net.forward(ln)   # finds output
  # t = time.time()
  # print('time=', t-t0)

  boxes = []
  confidences = []
  centroids = []
  results = []

  h, w = img.shape[:2]
  for output in outputs:  # Outputs have all the detection and their probability for every class
    for detection in output:    # detection is the the list of all probabilities with box dimension in start
      scores = detection[5:]  # everything in array after 5th element
      classID = np.argmax(scores)     # picks the maximum probability
      confidence = scores[classID] 

      if (confidence > threshold) and (classID == 0):
        #first 4 elemensts are box characteristics normalized to range(0,1)
        #first two element are middle co-ordinate
        # next two are width and height of blob           
        box = detection[:4] * np.array([w, h, w, h])    
        (centerX, centerY, width, height) = box.astype("int")   # typecasting to int, as array indexes are int
        x = int(centerX - (width / 2))      # finding upper corner
        y = int(centerY - (height / 2))
        box = [x, y, int(width), int(height)]   # changing origin to top left and typecasted to int
        boxes.append(box)                       # added the box to boxes
        confidences.append(float(confidence))   # added confidence to confidences
        centroids.append((centerX,centerY))

  indices = cv.dnn.NMSBoxes(boxes, confidences,score_threshold=threshold,nms_threshold=nms_threshold)
  # score_threshold -> threshold for confidence
  # nms_threshold -> threshold for how close to blobs are, if two blobs are too close, one of them is discarded
  # closeness is determined by IoU (intersection over Union)
  # discarding is based on confidence, higher confidence is retained

  if len(indices):
        for i in indices.flatten():
            x,y=boxes[i][0],boxes[i][1]
            w,h=boxes[i][2],boxes[i][3]

            r = (confidences[i], (x, y, x + w, y + h), centroids[i])
            results.append(r)

  return results

"""# **Bird's eye prespective (provisional)**"""

def birds_eye_view(corner_points,width,height,image):
  """
  Compute the transformation matrix
   corner_points : 4 corner points selected from the image
   height, width : size of the image
  return : transformation matrix and the transformed image
  """
  # Create an array out of the 4 corner points
  corner_points = np.float32(corner_points)
  # Create an array with the parameters (the dimensions) required to build the matrix
  img_params = np.float32([[0,0],[width,0],[0,height],[width,height]])
  # Compute and return the transformation matrix
  matrix = cv.getPerspectiveTransform(corner_points,img_params)
  img_transformed = cv.warpPerspective(image,matrix,(width,height))
  return matrix,img_transformed

"""# **Align the points to the Bird's eye prespective (provisional)**"""

def birds_eye_point(matrix,list_downoids):
  """ 
  Apply the perspective transformation to every ground point which has been detected on the main frame.
   matrix : the 3x3 matrix
   list_downoids : list that contains the points to transform
  return : list containing all the new points
  """
  # Compute the new coordinates of our points
  list_points_to_detect = np.float32(list_downoids).reshape(-1, 1, 2)
  transformed_points = cv.perspectiveTransform(list_points_to_detect, matrix)
  # Loop over the points and add them to the list that will be returned
  transformed_points_list = list()
  for i in range(0,transformed_points.shape[0]):
      transformed_points_list.append([transformed_points[i][0][0],transformed_points[i][0][1]])
  return transformed_points_list

"""# **File setup**"""

file_name = "pedestrians.mp4"
tot_frame = total_frames(file_name)
cap = cv.VideoCapture(file_name)

fourcc = cv.VideoWriter_fourcc(*'mp4v')
out = cv.VideoWriter('output.mp4',fourcc, 20.0,(1280,720))

# Load names of classes and get random colors
with open("coco.names") as f:
  classes=f.read().strip().split("\n")

"""# **Network setup**"""

# Give the configuration and weight files for the model and load the network.
net = cv.dnn.readNetFromDarknet('yolov3.cfg', 'yolov3.weights')  # Reads Network from .cfg and .weights
net.setPreferableBackend(cv.dnn.DNN_BACKEND_CUDA)   # this specifies what type of hardware to use (GPU or CPU)
net.setPreferableTarget(cv.dnn.DNN_TARGET_CUDA)     # sets preferable hardware

threshold = 0.5
nms_threshold = 0.4
distance_px=100   # arbitrary value for now but looked best

"""# **Process the input**"""

frame = 0

for i in tqdm (range (tot_frame), desc="Processing..."): 
  ret,img = cap.read()
  if not ret: break
        
  results=object_detection_YOLO(img, threshold, nms_threshold)
  # results = (confidence,bbox,centroid)
  # confidence -> confidence of the detected object
  # bbox -> top left and bottom right corner, 4 values list
  #centroid -> center of the bbox, 2 values list

  violate=set()

  if len(results)>=2:
        centres=np.array([r[2] for r in results])
        D=distance.cdist(centres,centres,metric="euclidean")
        # loop over the upper triangular of the distance matrix
        for i in range(0, D.shape[0]):
            for j in range(i + 1, D.shape[1]):
                # check to see if the distance between any two centroid pairs is less than the configured number of pixels
                if D[i, j] < distance_px:
                    # update our violation set with the indexes of the centroid pairs
                    violate.add(i)
                    violate.add(j)

  # loop over the results
  for (i, (prob, bbox, centroid)) in enumerate(results):
      # extract the bounding box and centroid coordinates, then initialize the color of the annotation
      (startX, startY, endX, endY) = bbox
      (cX, cY) = centroid
      color = (0, 255, 0) #green in BGR
      # if the index pair exists within the violation set, then update the color
      if i in violate:
          color = (0, 0, 255) #red in BGR
      cv.rectangle(img, (startX, startY), (endX, endY), color, 2) #draw bounding box
      cv.circle(img, (cX, cY), 5, color, 1) #draw center

  # draw the total number of social distancing violations on the output frame
  text = "Social Distancing Violations: {}".format(len(violate))
  cv.putText(img, text, (10, img.shape[0] - 25), cv.FONT_HERSHEY_SIMPLEX, 0.85, (0, 0, 255), 3)

  out.write(img)
  frame = frame + 1

cap.release()
out.release()
print("Processing Completed, Download output.mp4 to View Results")